{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "from functools import partial \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_dir = '/Users/yashwanthkaruparthi/Documents/Acads/sem7/design project/execution/data/solar_weather copy 2.csv'\n",
    "file_dir = '/Users/yashwanthkaruparthi/Documents/Acads/sem7/design project/execution/data/uae_data_2022.csv'\n",
    "time_step = 24\n",
    "\n",
    "# units = 250\n",
    "# learning_rate = 0.001\n",
    "# dropout_rate = 0.2\n",
    "# batch_size = 64\n",
    "\n",
    "num_feats = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# def create_dataset(dataset, time_step):\n",
    "#     print(f'dataset shape {dataset.shape}')\n",
    "#     dataX, dataY = [], []\n",
    "#     for i in range(len(dataset) - time_step):\n",
    "#         a = dataset[i:(i + time_step), :]  # Features: GHI and Energy delta\n",
    "#         dataX.append(a)\n",
    "#         dataY.append(dataset[i + time_step, 0])  # Target: Energy delta\n",
    "#     return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def create_dataset (X, y, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    \n",
    "    for i in range(len(X)-time_steps):\n",
    "        v = X[i:i+time_steps, :]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i+time_steps])\n",
    "        \n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def r2_metric(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    R-squared metric for regression tasks.\n",
    "    \"\"\"\n",
    "    ss_res = K.sum(K.square(y_true - y_pred))  # Residual sum of squares\n",
    "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))  # Total sum of squares\n",
    "    return 1 - (ss_res / (ss_tot + K.epsilon()))  # RÂ² formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unwanted code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(file_dir, header=0, infer_datetime_format=True, parse_dates=['Time'], index_col=['Time'])\n",
    "\n",
    "# # data = data[(data.index.month.isin([6, 7, 8])) & (data.index.year == 2021)]\n",
    "\n",
    "# data = data[(data.index.year == 2021)]\n",
    "\n",
    "# dataset = data[['Energy delta[Wh]', 'GHI', 'temp', 'pressure', 'humidity']]\n",
    "# # dataset = data[['Energy delta[Wh]', 'GHI']]\n",
    "\n",
    "# X = dataset.iloc[:, 1:].values  # Features\n",
    "# y = dataset.iloc[:, 0].values   # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_size = int(len(X) * 0.8)\n",
    "# test_size = len(X) - train_size\n",
    "# X_train, X_test = X[:train_size], X[train_size:]\n",
    "# y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# gbdt = GradientBoostingRegressor(n_estimators=100, learning_rate=learning_rate, max_depth=5)\n",
    "\n",
    "# gbdt.fit(X_train, y_train.ravel())\n",
    "\n",
    "# # Step 2: Get predictions for all models on both training and test sets\n",
    "# gbdt_output_train = gbdt.predict(X_train)\n",
    "# gbdt_output_test = gbdt.predict(X_test)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# gbdt_output_train_scaled = scaler.fit_transform(gbdt_output_train.reshape(-1, 1))\n",
    "# gbdt_output_test_scaled = scaler.transform(gbdt_output_test.reshape(-1, 1))\n",
    "\n",
    "# X_train_bilstm_gbdt, y_train_bilstm_gbdt = create_dataset(gbdt_output_train_scaled, time_step)\n",
    "# X_test_bilstm_gbdt, y_test_bilstm_gbdt = create_dataset(gbdt_output_test_scaled, time_step)\n",
    "\n",
    "# X_train_bilstm_gbdt = X_train_bilstm_gbdt.reshape(X_train_bilstm_gbdt.shape[0], time_step, 1)\n",
    "# X_test_bilstm_gbdt = X_test_bilstm_gbdt.reshape(X_test_bilstm_gbdt.shape[0], time_step, 1)\n",
    "\n",
    "# model = Sequential([\n",
    "#   Bidirectional(LSTM(int(units), activation='relu',return_sequences=True, input_shape=(time_step, num_feats))),\n",
    "#   Dropout(dropout_rate),\n",
    "#   Bidirectional(LSTM(int(units), activation='relu',return_sequences=False)),\n",
    "#   Dropout(dropout_rate),\n",
    "#   Dense(1)\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error', metrics=[r2_metric])\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train_bilstm_gbdt, y_train_bilstm_gbdt, epochs=10, batch_size=int(batch_size), validation_data=(X_test_bilstm_gbdt, y_test_bilstm_gbdt), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### woa bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n5/mlsmdgns0kqfybw1r9qm4n5w0000gp/T/ipykernel_7488/897731756.py:1: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  data = pd.read_csv(file_dir, header=0, infer_datetime_format=True, parse_dates=['Datetime'], index_col=['Datetime'])\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(file_dir, header=0, infer_datetime_format=True, parse_dates=['Datetime'], index_col=['Datetime'])\n",
    "\n",
    "year = 2022\n",
    "\n",
    "data = data[(data.index.year == year)]\n",
    "\n",
    "# dataset = data[['Temperature', 'DNI', 'DHI', 'GHI', 'Pressure', 'Wind Speed']]\n",
    "dataset = data[['Temperature', 'GHI', 'Pressure', 'Wind Speed']]\n",
    "\n",
    "train_size = int(len(dataset)*0.8)\n",
    "train_dataset, test_dataset = dataset.iloc[:train_size], dataset.iloc[train_size:]\n",
    "\n",
    "# Split train data to X and y\n",
    "X_trainy = train_dataset.drop('GHI', axis = 1)\n",
    "y_trainy = train_dataset.loc[:,['GHI']]\n",
    "\n",
    "# Split test data to X and y\n",
    "X_testy = test_dataset.drop('GHI', axis = 1)\n",
    "y_testy = test_dataset.loc[:,['GHI']]\n",
    "\n",
    "# Step 1: Train GBDT, XGB, and CatBoost on the training data\n",
    "gbdt = GradientBoostingRegressor(n_estimators=100, learning_rate=0.001, max_depth=5)\n",
    "\n",
    "gbdt.fit(X_trainy, y_trainy)\n",
    "\n",
    "# Step 2: Get predictions for all models on both training and test sets\n",
    "gbdt_output_train = gbdt.predict(X_trainy)\n",
    "gbdt_output_test = gbdt.predict(X_testy)\n",
    "\n",
    "input_scaler = MinMaxScaler()\n",
    "output_scaler = MinMaxScaler()\n",
    "\n",
    "input_scaler.fit(gbdt_output_train.reshape(-1, 1))\n",
    "# output_scaler.fit(gbdt_output_train.reshape(-1, 1))\n",
    "\n",
    "train_x_norm = input_scaler.transform(gbdt_output_train.reshape(-1, 1))\n",
    "test_x_norm = input_scaler.transform(gbdt_output_test.reshape(-1, 1))\n",
    "\n",
    "output_scaler.fit(y_trainy)\n",
    "\n",
    "train_y_norm = output_scaler.transform(y_trainy)\n",
    "test_y_norm = output_scaler.transform(y_testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the objective function\n",
    "def evaluate_bilstm(hyperparameters):\n",
    "    # Unpack hyperparameters\n",
    "    units, learning_rate, dropout_rate, batch_size = hyperparameters\n",
    "    # hyperparameters = [hyperparameters] if not isinstance(hyperparameters, (list, tuple)) else hyperparameters\n",
    "\n",
    "    # units, learning_rate, dropout_rate, batch_size = (hyperparameters + [None] * 4)[:4]\n",
    "\n",
    "    \n",
    "    # params = {\"units\": units, \"learning_rate\": learning_rate, \"dropout_rate\": dropout_rate, \"batch_size\": batch_size}\n",
    "    # missing_params = [name for name, value in params.items() if value is None]\n",
    "\n",
    "    # if missing_params: \n",
    "    #     raise ValueError(f'Missing required params: {\",\".join(missing_params)}')\n",
    "\n",
    "\n",
    "    print(units, learning_rate, dropout_rate, batch_size)\n",
    "\n",
    "\n",
    "    train_size = int(len(dataset)*0.8)\n",
    "    train_dataset, test_dataset = dataset.iloc[:train_size], dataset.iloc[train_size:]\n",
    "\n",
    "    # Split train data to X and y\n",
    "    X_trainy = train_dataset.drop('GHI', axis = 1)\n",
    "    y_trainy = train_dataset.loc[:,['GHI']]\n",
    "\n",
    "    # Split test data to X and y\n",
    "    X_testy = test_dataset.drop('GHI', axis = 1)\n",
    "    y_testy = test_dataset.loc[:,['GHI']]\n",
    "\n",
    "    gbdt = GradientBoostingRegressor(n_estimators=100, learning_rate=learning_rate, max_depth=5)\n",
    "\n",
    "    gbdt.fit(X_trainy, y_trainy)\n",
    "\n",
    "    # Step 2: Get predictions for all models on both training and test sets\n",
    "    gbdt_output_train = gbdt.predict(X_trainy)\n",
    "    gbdt_output_test = gbdt.predict(X_testy)\n",
    "\n",
    "    input_scaler = MinMaxScaler()\n",
    "    output_scaler = MinMaxScaler()\n",
    "\n",
    "    input_scaler.fit(gbdt_output_train.reshape(-1, 1))\n",
    "\n",
    "    train_x_norm = input_scaler.transform(gbdt_output_train.reshape(-1, 1))\n",
    "    test_x_norm = input_scaler.transform(gbdt_output_test.reshape(-1, 1))\n",
    "\n",
    "    output_scaler.fit(y_trainy)\n",
    "\n",
    "    train_y_norm = output_scaler.transform(y_trainy)\n",
    "    test_y_norm = output_scaler.transform(y_testy)\n",
    "\n",
    "    # gbdt_output_train_scaled = scaler.fit_transform(gbdt_output_train.reshape(-1, 1))\n",
    "    # gbdt_output_test_scaled = scaler.transform(gbdt_output_test.reshape(-1, 1))\n",
    "\n",
    "    TIME_STEPS = 24\n",
    "    X_train, y_train = create_dataset(train_x_norm, train_y_norm, TIME_STEPS)\n",
    "    X_test, y_test = create_dataset(test_x_norm, test_y_norm, TIME_STEPS)\n",
    "\n",
    "    # X_train_bilstm_gbdt, y_train_bilstm_gbdt = create_dataset(gbdt_output_train_scaled, time_step)\n",
    "    # X_test_bilstm_gbdt, y_test_bilstm_gbdt = create_dataset(gbdt_output_test_scaled, time_step)\n",
    "\n",
    "    # X_train_bilstm_gbdt = X_train_bilstm_gbdt.reshape(X_train_bilstm_gbdt.shape[0], time_step, 1)\n",
    "    # X_test_bilstm_gbdt = X_test_bilstm_gbdt.reshape(X_test_bilstm_gbdt.shape[0], time_step, 1)\n",
    "\n",
    "    model = Sequential([\n",
    "      Bidirectional(LSTM(int(units), activation='relu',return_sequences=True, input_shape=(time_step, num_feats))),\n",
    "      Dropout(dropout_rate),\n",
    "      Bidirectional(LSTM(int(units), activation='relu',return_sequences=False)),\n",
    "      Dropout(dropout_rate),\n",
    "      Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[r2_metric])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)\n",
    "    # Train the model\n",
    "    # history = model.fit(X_train_bilstm_gbdt, y_train_bilstm_gbdt, epochs=10, batch_size=int(batch_size), validation_data=(X_test_bilstm_gbdt, y_test_bilstm_gbdt), verbose=1)\n",
    "\n",
    "    # Return validation loss as fitness\n",
    "    return history.history['val_loss'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = [200, 1e-3, 0.1, 10]  # Lower bounds: [units, learning rate, dropout rate, batch size]\n",
    "ub = [600, 1e-2, 0.4, 300]  # Upper bounds: [units, learning rate, dropout rate, batch size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class WOA:\n",
    "    def __init__(self, obj_func, n_whale, spiral_constant, n_iter,\n",
    "                lb, ub):\n",
    "        self.obj_func = obj_func\n",
    "        self.n_whale = n_whale\n",
    "        self.spiral_constant = spiral_constant\n",
    "        self.n_iter = n_iter\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.whale = {}\n",
    "        self.prey = {}\n",
    "\n",
    "    def init_whale(self):\n",
    "        tmp = [np.random.uniform(self.lb, self.ub, size=(len(self.lb),))\n",
    "               for i in range(self.n_whale)]\n",
    "        self.whale['position'] = np.array(tmp)\n",
    "        self.whale['fitness'] = np.array([self.obj_func(pos) for pos in self.whale['position']])\n",
    "\n",
    "    def init_prey(self):\n",
    "        tmp = [np.random.uniform(self.lb, self.ub, size=(len(self.lb),))]\n",
    "        self.prey['position'] = np.array(tmp)\n",
    "        self.prey['fitness'] = self.obj_func(self.prey['position'])\n",
    "\n",
    "    def update_prey(self):\n",
    "        if self.whale['fitness'].min() < self.prey['fitness'][0]:\n",
    "            self.prey['position'][0] = self.whale['position'][self.whale['fitness'].argmin()]\n",
    "            self.prey['fitness'][0] = self.whale['fitness'].min()\n",
    "\n",
    "    def search(self, idx, A, C):\n",
    "        random_whale = self.whale['position'][np.random.randint(low=0, high=self.n_whale,\n",
    "                                                                size=len(idx[0]))]\n",
    "        d = np.abs(C[..., np.newaxis] * random_whale - self.whale['position'][idx])\n",
    "        self.whale['position'][idx] = np.clip(random_whale - A[..., np.newaxis] * d, self.lb, self.ub)\n",
    "\n",
    "    def encircle(self, idx, A, C):\n",
    "        d = np.abs(C[..., np.newaxis] * self.prey['position'] - self.whale['position'][idx])\n",
    "        self.whale['position'][idx] = np.clip(self.prey['position'][0] - A[..., np.newaxis] * d, self.lb, self.ub)\n",
    "\n",
    "    def bubble_net(self, idx):\n",
    "        d_prime = np.abs(self.prey['position'] - self.whale['position'][idx])\n",
    "        l = np.random.uniform(-1, 1, size=len(idx[0]))\n",
    "        self.whale[\"position\"][idx] = np.clip(\n",
    "            d_prime * np.exp(self.spiral_constant * l)[..., np.newaxis] * np.cos(2 * np.pi * l)[..., np.newaxis]\n",
    "            + self.prey[\"position\"],\n",
    "            self.lb,\n",
    "            self.ub,\n",
    "        )\n",
    "\n",
    "    def optimize(self, a):\n",
    "\n",
    "        p = np.random.random(self.n_whale)\n",
    "        r1 = np.random.random(self.n_whale)\n",
    "        r2 = np.random.random(self.n_whale)\n",
    "        A = 2 * a * r1 - a\n",
    "        C = 2 * r2\n",
    "        search_idx = np.where((p < 0.5) & (abs(A) > 1))\n",
    "        encircle_idx = np.where((p < 0.5) & (abs(A) <= 1))\n",
    "        bubbleNet_idx = np.where(p >= 0.5)\n",
    "        self.search(search_idx, A[search_idx], C[search_idx])\n",
    "        self.encircle(encircle_idx, A[encircle_idx], C[encircle_idx])\n",
    "        self.bubble_net(bubbleNet_idx)\n",
    "        self.whale['fitness'] = self.obj_func(self.whale['position'])\n",
    "\n",
    "    def run(self):\n",
    "        self.init_whale()\n",
    "        self.init_prey()\n",
    "        f_values = [self.prey['fitness'][0]]\n",
    "        for n in range(self.n_iter):\n",
    "            #print(\"Iteration = \", n, \" f(x) = \", self.prey['fitness'][0])\n",
    "            a = 2 - n * (2 / self.n_iter)\n",
    "            self.optimize(a)\n",
    "            self.update_prey()\n",
    "            f_values.append(self.prey['fitness'][0])\n",
    "        optimal_x = self.prey['position'].squeeze()\n",
    "        return f_values, optimal_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216.2375368171455 0.00492850079135253 0.39435588633525953 124.05873887048331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected int32 passed to parameter 'size' of op 'Slice', got [496.23495548193324] of type 'list' instead. Error: Expected int32, but got 496.23495548193324 of type 'float64'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m woa \u001b[38;5;241m=\u001b[39m WOA(obj_func\u001b[38;5;241m=\u001b[39mobj_func, n_whale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, spiral_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, lb\u001b[38;5;241m=\u001b[39mlb, ub\u001b[38;5;241m=\u001b[39mub)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Run WOA optimization\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m f_values, optimal_hyperparameters \u001b[38;5;241m=\u001b[39m \u001b[43mwoa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal Hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimal_hyperparameters)\n",
      "Cell \u001b[0;32mIn[41], line 70\u001b[0m, in \u001b[0;36mWOA.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_whale\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_prey()\n\u001b[1;32m     72\u001b[0m     f_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprey[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfitness\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[41], line 22\u001b[0m, in \u001b[0;36mWOA.init_whale\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mub, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlb),))\n\u001b[1;32m     20\u001b[0m        \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_whale)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhale[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tmp)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhale[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfitness\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhale\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn[41], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m tmp \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mub, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlb),))\n\u001b[1;32m     20\u001b[0m        \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_whale)]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhale[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tmp)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhale[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfitness\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhale[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[42], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Partial function to adapt objective function for WOA\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# obj_func = partial(lambda params: [evaluate_bilstm(param) for param in params])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# obj_func = lambda params: evaluate_bilstm(params)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m obj_func \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28;01mlambda\u001b[39;00m params: \u001b[43mevaluate_bilstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Instantiate WOA\u001b[39;00m\n\u001b[1;32m      9\u001b[0m woa \u001b[38;5;241m=\u001b[39m WOA(obj_func\u001b[38;5;241m=\u001b[39mobj_func, n_whale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, spiral_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, lb\u001b[38;5;241m=\u001b[39mlb, ub\u001b[38;5;241m=\u001b[39mub)\n",
      "Cell \u001b[0;32mIn[39], line 75\u001b[0m, in \u001b[0;36mevaluate_bilstm\u001b[0;34m(hyperparameters)\u001b[0m\n\u001b[1;32m     65\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m     66\u001b[0m   Bidirectional(LSTM(\u001b[38;5;28mint\u001b[39m(units), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(time_step, num_feats))),\n\u001b[1;32m     67\u001b[0m   Dropout(dropout_rate),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m   Dense(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     71\u001b[0m ])\n\u001b[1;32m     73\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[r2_metric])\n\u001b[0;32m---> 75\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# history = model.fit(X_train_bilstm_gbdt, y_train_bilstm_gbdt, epochs=10, batch_size=int(batch_size), validation_data=(X_test_bilstm_gbdt, y_test_bilstm_gbdt), verbose=1)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Return validation loss as fitness\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/op_def_library.py:561\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    560\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m passed to parameter \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_arg\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of op \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_type_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(values)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m   \u001b[38;5;66;03m# What type does convert_to_tensor think it has?\u001b[39;00m\n\u001b[1;32m    568\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected int32 passed to parameter 'size' of op 'Slice', got [496.23495548193324] of type 'list' instead. Error: Expected int32, but got 496.23495548193324 of type 'float64'."
     ]
    }
   ],
   "source": [
    "# Partial function to adapt objective function for WOA\n",
    "# obj_func = partial(lambda params: [evaluate_bilstm(param) for param in params])\n",
    "\n",
    "# obj_func = lambda params: evaluate_bilstm(params)\n",
    "\n",
    "obj_func = partial(lambda params: evaluate_bilstm(params))\n",
    "\n",
    "# Instantiate WOA\n",
    "woa = WOA(obj_func=obj_func, n_whale=10, spiral_constant=1, n_iter=20, lb=lb, ub=ub)\n",
    "\n",
    "# Run WOA optimization\n",
    "f_values, optimal_hyperparameters = woa.run()\n",
    "\n",
    "print(\"Optimal Hyperparameters:\", optimal_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
